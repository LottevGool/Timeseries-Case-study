Person A — Data Exploration + ARIMA + Reporting

This person handles everything related to understanding the data and the non-seasonal ARIMA models.

Tasks

Data exploration
– Import data
– Visualize demand (overall + seasonal patterns)
– Stationarity discussion
– ACF/PACF plots
– Explain why trips ≠ true demand
(explicitly required; cite the assignment from 

EBC4008-case1-2025 (1)

)

Non-seasonal ARIMA models
– manual ARIMA selection (ACF/PACF)
– auto.arima approach
– explain ARIMA theory on slides
– provide clear R code + output interpretation

Part of the presentation
– slides on ARIMA
– results for ARIMA
– interpretation of in-sample AIC/BIC + residuals

Why this is good

These tasks require a strong intro section + structured thinking + well-explained R scripts.
It’s self-contained and does not interfere with the seasonal models.

Person B — SARIMA + ETS (Exponential Smoothing) + Reporting
Tasks

Seasonality modelling
– Transform series into ts object with correct frequency
– Explain seasonality patterns
– Explain SARIMA theory
– Fit several SARIMA models
– Analyze outputs

ETS models (Exponential Smoothing)
– Introduce ETS (AAA, MAM, etc.)
– Fit ETS using ets()
– Compare with SARIMA & ARIMA

Part of the presentation
– slides introducing SARIMA and ETS
– explain differences and similarities of SARIMA vs ETS
– present model selection results

Why this is good

Seasonal modelling (SARIMA & ETS) forms one coherent block.
This avoids mixing responsibilities and ensures consistent seasonal analysis.

Person C — Forecasting (Rolling & Expanding) + Metrics + Tests (DM/MCS)

This is the most technical part — perfect for a team member who is comfortable with loops + evaluation.

Tasks

Implement rolling window forecasting
– Write the for-loop manually (assignment explicitly requires not using caret)
– horizon h = 24
– store forecasts for all models

Implement expanding window forecasting
– modify the loop
– store forecasts
– finalize forecast results

Forecast accuracy metrics
– RMSE, MAE, MAPE
– justify metric choice (based on cost of over/under-prediction)

Forecast comparison tests
– DM test (Diebold–Mariano)
– MCS test
– Provide interpretation

Part of the presentation
– slides explaining rolling vs expanding windows
– slides explaining RMSE/MAE/MAPE
– slides explaining DM and MCS
– which model performed best out-of-sample
